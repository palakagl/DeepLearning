{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TACO_Dataset_Cat_5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqGIpqZ0RcPyhfh7OHDbYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/DeepLearning/blob/main/TACO_Dataset_Cat_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKE5nTxH8i3K"
      },
      "outputs": [],
      "source": [
        "# Uncomment only for first run\n",
        "!pip install neptune-client neptune-tensorflow-keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "import neptune.new as neptune\n",
        "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow      # To show images using cv2 module\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "metadata": {
        "id": "vsbMBK079IvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive so dataset can be accessed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KxRyo1DV9RDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = neptune.init(\n",
        "    project=\"palakagl/TACO-Dataset\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwYjM0NzcxZi0xYjY1LTRmZWQtOTNmNC1iNjYyNTdkYTBmMWYifQ==\",\n",
        ") "
      ],
      "metadata": {
        "id": "Y627smYk-K3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Select base model  for transfer learning\n",
        "  model_name = \"mobilenet_v2_140_224\" # @param [ 'efficientnet_v2_50', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152',  'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "  model_handle_map = {\n",
        "    \"efficientnet_v2_50\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",   \n",
        "    \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "    \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "    \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "    \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "    \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",\n",
        "    \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "    \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "    \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "    \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "    \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "    \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "    \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "    \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "  }\n",
        "\n",
        "  PIXELS = 224#@param {type:\"integer\"}\n",
        "  BATCH_SIZE = 16#@param {type:\"integer\"}\n",
        "\n",
        "  model_handle = model_handle_map.get(model_name)\n",
        "\n",
        "  print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "  IMAGE_SIZE = (PIXELS, PIXELS)\n",
        "  print(f\"Input size {IMAGE_SIZE}\")\n"
      ],
      "metadata": {
        "id": "Os3nb_RR-RoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Log to NeptuneAI\n",
        "run[\"model/transferlearning/model_name\"] = model_name\n",
        "#run[\"model/transferlearning/model_url\"] = model_handle\n",
        "run[\"model/transferlearning/model_batch_size\"] = BATCH_SIZE\n",
        "run[\"model/parameters/image_size\"] = IMAGE_SIZE"
      ],
      "metadata": {
        "id": "Fbe0MvKQ-WXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(save_dir=\"./\"):\n",
        "\n",
        "  train_data_path = '/content/drive/MyDrive/Colab Notebooks/data/trash_grouped/train_preprocessed/'\n",
        "  test_data_path = '/content/drive/MyDrive/Colab Notebooks/data/trash_grouped/test_preprocessed/'\n",
        "\n",
        "  datagen_kwargs = dict(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "\n",
        "  dataflow_kwargs = dict(target_size=(PIXELS, PIXELS), \n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        class_mode=\"categorical\",\n",
        "                        x_col='file_name',\n",
        "                        y_col='labels')\n",
        "\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      horizontal_flip=True, vertical_flip=True, rotation_range=90,\n",
        "      **datagen_kwargs)\n",
        "  train_generator = train_datagen.flow_from_dataframe(\n",
        "      grouped_image_dataframe,\n",
        "      directory=train_data_path,\n",
        "      subset='training',\n",
        "      **dataflow_kwargs\n",
        "  )\n",
        "\n",
        "\n",
        "  val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      **datagen_kwargs)\n",
        "  val_generator = val_datagen.flow_from_dataframe(\n",
        "      grouped_image_dataframe,\n",
        "      directory=train_data_path,\n",
        "      subset='validation',\n",
        "      **dataflow_kwargs\n",
        "  )\n",
        "  return val_generator, train_generator\n",
        "\n",
        "VAL_generator, TRAIN_generator = load_data()"
      ],
      "metadata": {
        "id": "XIM_rTiA-aHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the first 9 images in the planet dataset\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.image import imread\n",
        "# define location of dataset\n",
        "folder = '/content/drive/MyDrive/Colab Notebooks/data/trash_grouped/train_preprocessed/'\n",
        "# plot first few images\n",
        "for i in range(4):\n",
        "\t# define subplot\n",
        "\tpyplot.subplot(330 + 1 + i)\n",
        "\t# define filename\n",
        "\tfilename = folder + 'batch_1_0000' + str(i+3) + '.jpg'\n",
        "\t# load image pixels\n",
        "\timage = imread(filename)\n",
        "\t# plot raw pixel data\n",
        "\tpyplot.imshow(image)\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "5F3SRfEA-ulH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "\n",
        "def define_model(in_shape=(224, 224, 3), out_shape=59):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(out_shape, activation='sigmoid'))\n",
        "\treturn model\n",
        " \n",
        "model = define_model()"
      ],
      "metadata": {
        "id": "NUfNH2LF-vuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_with_transfer_learning(trainable=False, params={}):\n",
        "\n",
        "    #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "   base_model=keras.applications.MobileNetV2(weights='imagenet',\n",
        "                                             include_top=False,\n",
        "                                            input_shape=IMAGE_SIZE+(3,))\n",
        "\n",
        "   base_model.trainable = False                                      \n",
        "\n",
        "   inputs = keras.Input(shape=IMAGE_SIZE+(3,))\n",
        "   x = base_model(inputs, training=False)\n",
        "   x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "   x=Dense(params['dense_units_1'],activation=params['dense_layer_activation_1'])(x)\n",
        "   x=Dropout(params['dropout_1'])(x)\n",
        "   x=Dense(params['dense_units_2'],activation=params['dense_layer_activation_2'])(x)\n",
        "   x=Dropout(params['dropout_2'])(x)\n",
        "   x = Flatten()(x)\n",
        "   x=Dense(params['dense_units_2'],activation=params['dense_layer_activation_2'])(x)\n",
        "   outputs = keras.layers.Dense(59, activation=params['final_layer_activation'])(x)\n",
        "   model = keras.Model(inputs, outputs)\n",
        "\n",
        "   model.summary()\n",
        "   return model\n",
        "\n",
        "params = {\n",
        "    \"dense_units_1\": 512,\n",
        "    \"dense_layer_activation_1\": \"relu\",\n",
        "    \"dropout_1\": 0.3,    \n",
        "    \"dense_units_2\": 128,\n",
        "    \"dense_layer_activation_2\": \"relu\",\n",
        "    \"dropout_2\": 0.1,\n",
        "    \"final_layer_activation\": \"sigmoid\",\n",
        "    \"learning_rate\": 1e-2,\n",
        "    \"n_epochs\": 50,    \n",
        "    \"regularizer_rate\":0.005\n",
        "}\n",
        "run[\"model/parameters\"] = params\n",
        "#model = build_model_with_transfer_learning(params=params)"
      ],
      "metadata": {
        "id": "rX4l58sg-1lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend\n",
        " \n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score"
      ],
      "metadata": {
        "id": "MFLDdwQU-5cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), \n",
        "  loss=keras.losses.BinaryCrossentropy(),\n",
        "  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = compile_model(model)"
      ],
      "metadata": {
        "id": "DSAelEi7_CPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Callbacks\n",
        "\n",
        "# Model Checkpoint\n",
        "tl_checkpoint_1 = ModelCheckpoint(filepath = '/content/drive/MyDrive/Colab Notebooks/Models/garbageclassification_model_checkpoint/resnet_best_weights.hdf5', save_best_only = True, verbose = 0)\n",
        "\n",
        "# Early Stopping Checkpoint\n",
        "EarlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)"
      ],
      "metadata": {
        "id": "iDw0ORan_Da5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs):\n",
        "    steps_per_epoch = TRAIN_generator.samples // TRAIN_generator.batch_size\n",
        "    validation_steps = VAL_generator.samples // VAL_generator.batch_size\n",
        "\n",
        "    history = model.fit(\n",
        "      TRAIN_generator,\n",
        "      epochs=num_epochs, steps_per_epoch=steps_per_epoch,\n",
        "      validation_data = VAL_generator,\n",
        "      validation_steps = validation_steps,\n",
        "      callbacks=[NeptuneCallback(run=run),EarlyStoppingCallback]).history\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, num_epochs=50)"
      ],
      "metadata": {
        "id": "TBPfN_KZ_Hl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"fbeta (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(history[\"fbeta\"])\n",
        "plt.plot(history[\"val_fbeta\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(history[\"accuracy\"])\n",
        "plt.plot(history[\"val_accuracy\"])"
      ],
      "metadata": {
        "id": "8qRDGfDv_Lyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define filename\n",
        "filename = '/content/drive/MyDrive/Colab Notebooks/data/trash/Aerosol/batch_1_000054.jpg'\n",
        "\n",
        "img = tf.keras.preprocessing.image.load_img(\n",
        "    filename, target_size=(224, 224)\n",
        ")\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "print(predictions)\n",
        "score = tf.nn.sigmoid(predictions[0])\n",
        "print(score)\n",
        "print(list(class_names))\n",
        "print( \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(list(class_names)[np.argmax(score)], 100 * np.max(score)) )"
      ],
      "metadata": {
        "id": "AXiJIk6A_O6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/Models/garbageclassification_model_checkpoint/resnet_best_weights.hdf5') # initialize the best trained weights\n",
        "preds = model.predict(test_ds)\n",
        "pred_classes = np.argmax(preds, axis = 1)"
      ],
      "metadata": {
        "id": "bgylmfKM_Sos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"/content/drive/MyDrive/Colab Notebooks/Models/saved_garbageclassification_model\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "metadata": {
        "id": "e8wIMa7V_VcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aAg8YNVA_ZJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}