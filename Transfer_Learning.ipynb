{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ25iAgAOJ5K"
      },
      "source": [
        "# MMAI 894 - Exercise 3\n",
        "## Transfer learning with DistilBert\n",
        "The goal of this excercise is to build a text classifier using the pretrained DistilBert published by HuggingFace. You will be doing this using the Glue/CoLA dataset (https://nyu-mll.github.io/CoLA/).\n",
        "\n",
        "Submission instructions:\n",
        "\n",
        "- You cannot edit this notebook directly. Save a copy to your drive, and make sure to identify yourself in the title using name and student number\n",
        "- Do not insert new cells before the final one (titled \"Further exploration\") \n",
        "- Verify that your notebook can _restart and run all_. \n",
        "- Unlike previous assignments, please **submit all three formats: .py, .ipynb, and html** (see https://torbjornzetterlund.com/how-to-save-a-google-colab-notebook-as-html/)\n",
        " - The notebook and html submissions should show the completion of your best performing run\n",
        " - Submission files should be named: `studentID_lastname_firstname_ex3.py (or .html, .ipynb)`\n",
        "- The mark will be assessed on the implementation of the functions with #TODO\n",
        "- **Do not change anything outside the functions**  unless in the further exploration section\n",
        "- - As you are encouraged to explore the network configuration, 20% of the mark is based on final accuracy. \n",
        "- Note: You do not have to answer the questions in thie notebook as part of your submission. They are meant to guide you.\n",
        "\n",
        "- You should not need to use any additional libraries other than the ones listed below. You may want to import additional modules from those libraries, however."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hZj4c0xTeMwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9696904-1ee2-4799-d342-bde7284fa364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "# This cell installs and sets up DistilBert import, as well as the dataset, which we will \n",
        "# use tf.datasets to load (https://www.tensorflow.org/datasets/catalog/overview)\n",
        "\n",
        "!pip install -q transformers tfds-nightly\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm \n",
        "!pip install scikit-learn  -U\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "\n",
        "try: # this is only working on the 2nd try in colab :)\n",
        "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "except Exception as err: # so we catch the error and import it again\n",
        "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "import spacy, re\n",
        "from spacy.attrs import ORTH\n",
        "from spacy.lang.en.examples import sentences\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHvJJjnCRYF2"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q3gYLKfEd0Hb"
      },
      "outputs": [],
      "source": [
        "def load_data(save_dir=\"./\"):\n",
        "    dataset = tfds.load('glue/cola', shuffle_files=True)\n",
        "    train = tfds.as_dataframe(dataset[\"train\"])\n",
        "    val = tfds.as_dataframe(dataset[\"validation\"])\n",
        "    test = tfds.as_dataframe(dataset[\"test\"])\n",
        "    return train, val, test\n",
        "\n",
        "def prepare_raw_data(df):\n",
        "    raw_data = df.loc[:, [\"idx\", \"sentence\", \"label\"]]\n",
        "    raw_data[\"label\"] = raw_data[\"label\"].astype('category')\n",
        "    return raw_data\n",
        "\n",
        "train, val, test = load_data()\n",
        "train = prepare_raw_data(train)\n",
        "val = prepare_raw_data(val)\n",
        "test = prepare_raw_data(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQC3J5brmZdy"
      },
      "source": [
        "Before using this data, we need to clean and QA it. Unlike MNIST, this is a text dataset, and we should be more caerful. For example:\n",
        "- Are there any duplicate entries? \n",
        "- What is the range of lengths for the sentences? Should we impose a minimum sentence length?\n",
        "- Are there \"non-sentence\" entries? For example, hashtags or other features we should remove? (luckily, this dataset is quite clean, but that might not always be the case!)\n",
        "\n",
        "NOTE! The sentences are encoded as binary strings. To do text manipulations, you might need to decode them using `s.decode(\"utf-8\")`\n",
        "\n",
        "You may notice that that test set has no labels. This is because Glue is a benchmark dataset, and only gets scored on submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xHNJC2vZmTWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fde60e5-37c4-4af3-db69-47fe773d0949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    idx                                           sentence label\n",
            "0  1680  b'It is this hat that it is certain that he wa...     1\n",
            "1  1456  b'Her efficient looking up of the answer pleas...     1\n",
            "2  4223          b'Both the workers will wear carnations.'     1\n",
            "3  4093  b'John enjoyed drawing trees for his syntax ho...     1\n",
            "4  7111  b'We consider Leslie rather foolish, and Lou a...     1\n",
            "    idx                                         sentence label\n",
            "0   163            b'Brian was wiping behind the stove.'    -1\n",
            "1   131       b'You could give a headache to a Tylenol.'    -1\n",
            "2  1021                          b'I want to meet at 6.'    -1\n",
            "3   166                        b'Packages carry easily.'    -1\n",
            "4  1039  b\"Many people said they were sick who weren't.\"    -1\n"
          ]
        }
      ],
      "source": [
        "def clean_data(df):\n",
        "\n",
        "#   # TODO: What data cleaning/filtering should you consider?\n",
        "#   # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "  spacy_lang = spacy.load(\"en_core_web_sm\")\n",
        "  clean_sentence = []\n",
        "  #remove duplicate entries\n",
        "  clean_df = df.drop_duplicates(subset=[\"sentence\", \"label\"], keep='first')\n",
        "  \n",
        "  lemma_dict = { \n",
        "      \"yrs\":  \"year\",\"hrs\":  \"hour\",\"nd\":  \"and\",\"avg\":  \"average\",\"bc\":  \"because\",\"cc\":  \"credit card\",\"cu\":  \"credit union\",\"cuz\":  \"because\",\"dept\":  \"department\",\n",
        "      \"dunno\":  \"do not know\",\"em\":  \"them\",\"goo\":  \"good\",\"idk\":  \"i do not know\",\"info\":  \"information\",\"lo\":  \"low\",\"prob\":  \"problem\",\"probs\":  \"problem\",\n",
        "      \"refi\":  \"refinance\",\"thay\":  \"they\",\"thye\":  \"they\",\"yr\":  \"year\",\"yrs\":  \"year\",\"cant\":  \"can not\",\"doesnt\":  \"do not\",\"dont\":  \"do not\",\"havent\":  \"have not\",\n",
        "      \"im\":  \"i be\",\"didnt\":  \"do not\",\"isnt\":  \"be not\",\"ive\":  \"i have\",\"thats\":  \"that be\",\"theres\":  \"there be\",\"wont\":  \"will not\",\"cust\":  \"customer\",\n",
        "      \"dk\":  \"do not know\",\"haven\":  \"have not\",\"ain't\":  \"am not\",\"aren't\":  \"are not\",\"can't\":  \"cannot\",\"can't've\":  \"cannot have\",\"'cause\":  \"because\",\"'coz\":  \"because\",\n",
        "      \"could've\":  \"could have\",\"couldn't\":  \"could not\", \"couldn't've\":  \"could not have\",\"didn't\":  \"did not\",\"doesn't\":  \"does not\",\"don't\":  \"do not\",\"hadn't\":  \"had not\",\n",
        "      \"hadn't've\":  \"had not have\",\"hasn't\":  \"has not\", \"haven't\":  \"have not\",\"havent\":  \"have not\",  \"he'd\":  \"he would\",\"he'd've\":  \"he would have\",\"he'll\":  \"he will\",\n",
        "      \"he'll've\":  \"he shall have\",\"he's\":  \"he is\",\"how'd\":  \"how did\",\"how'd'y\":  \"how do you\",\"how'll\":  \"how will\",\"how's\":  \"how is\",\"i'd\":  \"I would\",\"i'd've\":  \"I would have\",\n",
        "      \"i'll\":  \"I will\", \"i'll've\":  \"I will have\",\"i'm\":  \"I am\",\"i've\":  \"I have\",\"isn't\":  \"is not\",\"it'd\":  \"it would\",\"it'd've\":  \"it would have\",\"it'll\":  \"it will\",\n",
        "      \"it'll've\":  \"it will have\",\"it's\":  \"it is\",\"let's\":  \"let us\",\"ma'am\":  \"madam\",\"mayn't\":  \"may not\",\"might've\":  \"might have\",\"mightn't\":  \"might not\",\n",
        "      \"mightn't've\":  \"might not have\",\"must've\":  \"must have\",\"mustn't\":  \"must not\",\"mustn't've\":  \"must not have\",\"needn't\":  \"need not\",\"needn't've\":  \"need not have\",\n",
        "      \"o'clock\":  \"of the clock\",\"oughtn't\":  \"ought not\",\"oughtn't've\":  \"ought not have\",\"shan't\":  \"shall not\",\"sha'n't\":  \"shall not\",\"shan't've\":  \"shall not have\",\n",
        "      \"she'd\":  \"she would\",\"she'd've\":  \"she would have\",\"she'll\":  \"she will\",\"she'll've\":  \"she will have\",\"she's\":  \"she is\",\"should've\":  \"should have\",\"shouldn't\":  \"should not\",\n",
        "      \"shouldn't've\":  \"should not have\",\"so've\":  \"so have\",\"so's\":  \"so is\",\"that'd\":  \"that would\",\"that'd've\":  \"that would have\",\"that's\":  \"that is\",\"there'd\":  \"there would\",\n",
        "      \"there'd've\":  \"there would have\",\"there's\":  \"there is\",\"they'd\":  \"they would\",\"they'd've\":  \"they would have\",\"they'll\":  \"they will\",\"they'll've\":  \"they will have\",\"they're\":  \"they are\",\n",
        "      \"they've\":  \"they have\",\"to've\":  \"to have\",\"wasn't\":  \"was not\",\"we'd\":  \"we would\",\"we'd've\":  \"we would have\",\"we'll\":  \"we will\",\"we'll've\":  \"we will have\",\"we're\":  \"we are\",\n",
        "      \"we've\":  \"we have\",\"weren't\":  \"were not\",\"what'll\":  \"what will\",\"what'll've\":  \"what will have\",\"what're\":  \"what are\",\"what's\":  \"what is\",\"what've\":  \"what have\",\n",
        "      \"when's\":  \"when is\",\"when've\":  \"when have\",\"where'd\":  \"where did\",\"where's\":  \"where is\",\"where've\":  \"where have\",\"who'll\":  \"who will\",\"who'll've\":  \"who will have\",\"who's\":  \"who is\",\n",
        "      \"who've\":  \"who have\",\"why's\":  \"why is\",\"why've\":  \"why have\",\"will've\":  \"will have\",\"won't\":  \"will not\",\"won't've\":  \"will not have\",\"would've\":  \"would have\",\"wouldn't\":  \"would not\",\n",
        "      \"wouldn't've\":  \"would not have\",\"y'all\":  \"you all\",\"y'all'd\":  \"you all would\",\"y'all'd've\":  \"you all would have\",\"y'all're\":  \"you all are\",\"y'all've\":  \"you all have\",\"you'd\":  \"you would\",\n",
        "      \"you'd've\":  \"you would have\",\"you'll\":  \"you will\",\"you'll've\":  \"you will have\",\"you're\":  \"you are\",\"you've\":  \"you have\",\"4g\":  \"Fourth Gen\",  \"4G\":  \"Fourth Gen\",\"3g\":  \"Third Gen\",\n",
        "      \"3G\":  \"Third Gen\",\"isnt\":  \"is not\",\"isn\": \"isnot\",\"didn\": \"did not\",\"didnt\":  \"did not\",\"nt\": \"not\",\"co\": \"company\",\"cos\": \"because\",\"ve\": \"have\",\"doesn\": \"does not\",\"m\": \"am\",\"nt\": \"not\",\"re\":  \"are\",\n",
        "      \"dint\":  \"did not\",\"dont\":  \"do not\",\"ok \":  \"okay\",\"cust \":  \"customer\",\"cuz \":  \"because\",\"app \":  \"application\",\"& \":  \"and\"\n",
        "  }\n",
        "\n",
        "  for key in lemma_dict:\n",
        "    case = [{ORTH: key}]\n",
        "    spacy_lang.tokenizer.add_special_case(key, case)\n",
        "  \n",
        "  for text in df.sentence:\n",
        "    clean_str = ''\n",
        "    substituted_text = ''\n",
        "    #lower_case for spacy substitutions\n",
        "    clean_str = text.lower().decode('utf-8')\n",
        "    #remove letter repetition (if more than 2)\n",
        "    clean_str = re.sub(r'([a-z])\\1{2,}', r'\\1', clean_str)\n",
        "    #remove phrase repetition\n",
        "    clean_str = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', clean_str)\n",
        "    \n",
        "    #replace contractions with custom substitution and lemmatization\n",
        "    clean_str = spacy_lang(clean_str)\n",
        "    for word in clean_str:\n",
        "      if word.is_stop == False:\n",
        "        word = word.text if word.lemma_ == '-PRON-' else word.lemma_\n",
        "        word.replace(\"â€™\",\"\\'\")\n",
        "        word.replace(\"'s\",\"\")\n",
        "        word.replace(\"â\",\"\\'\")\n",
        "        word.replace(\"b'\",\"\")\n",
        "        word = lemma_dict.get(word, word)\n",
        "        word = re.sub('[^a-z]+', '', word)\n",
        "        substituted_text += word + ' '\n",
        "        \n",
        "    \n",
        "    clean_sentence.append(re.sub('\\s\\s+',' ', substituted_text).strip())\n",
        "    \n",
        "  clean_data = pd.concat([clean_df.idx, pd.Series(clean_sentence).to_frame(name='sentence'), clean_df.label],axis = 1).dropna()\n",
        "\n",
        "  return clean_data\n",
        "\n",
        "#commenting out this part as it's repition of the same process in the next cell\n",
        "# train = clean_data(train)\n",
        "# val = clean_data(val)\n",
        "# test = clean_data(test)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAYstlfPQSvd"
      },
      "source": [
        "Next, we need to prepare the text for DistilBert. Instead of ingesting raw text, the model uses token IDs to map to internal embedding. Additionally, since the input is fixed size (due to our use of batches), we need to let the model know which tokens to use (i.e. are part of the sentence).\n",
        "\n",
        "Luckily, `dbert_tokenizer` takes care of all that for us - \n",
        "- Preprocessing: https://huggingface.co/transformers/preprocessing.html\n",
        "- Summary of tokenizers (DistilBert uses WordPiece): https://huggingface.co/transformers/tokenizer_summary.html#wordpiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_RBthPA0fcTA"
      },
      "outputs": [],
      "source": [
        "def extract_text_and_y(df):\n",
        "  #text = [x.decode('utf-8') for x in  df.sentence.values] #already decoded to 'utf-8' in text cleaning function, for applying re substituion\n",
        "  text = list(df.sentence.values)\n",
        "  # for multiclass problems, you can use sklearn.preprocessing.OneHotEncoder, but we only have two classes, so we'll use a single sigmoid output\n",
        "  y = np.array([x for x in df.label.values])\n",
        "  return text, y\n",
        "\n",
        "def encode_text(text):\n",
        "    # TODO: encode text using dbert_tokenizer\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    for text in text:\n",
        "      encoded_dict = dbert_tokenizer.encode_plus(\n",
        "          text,                      # Sentence to encode.\n",
        "          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "          max_length = 25,           # Pad & truncate all sentences.\n",
        "          truncation = True,\n",
        "          padding = 'max_length',                      \n",
        "          return_attention_mask = True   # Construct attn. masks.\n",
        "          )\n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_mask.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = np.asarray(input_ids)\n",
        "    attention_mask = np.array(attention_mask)\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# the following prepares the input for running in DistilBert\n",
        "train_text, train_y = extract_text_and_y(clean_data(train))\n",
        "val_text, val_y = extract_text_and_y(clean_data(val))\n",
        "test_text, test_y = extract_text_and_y(clean_data(test))\n",
        "\n",
        "train_input, train_mask = encode_text(train_text)\n",
        "val_input, val_mask = encode_text(val_text)\n",
        "test_input, test_mask = encode_text(test_text)\n",
        "\n",
        "train_model_inputs_and_masks = {\n",
        "    'inputs' : train_input,\n",
        "    'masks' : train_mask\n",
        "}\n",
        "\n",
        "val_model_inputs_and_masks = {\n",
        "    'inputs' : val_input,\n",
        "    'masks' : val_mask\n",
        "}\n",
        "\n",
        "test_model_inputs_and_masks = {\n",
        "    'inputs' : test_input,\n",
        "    'masks' : test_mask\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2USajN2MWjn"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "## Build and Train Model\n",
        "\n",
        "Resources:\n",
        "- BERT paper https://arxiv.org/pdf/1810.04805.pdf\n",
        "- DistilBert paper: https://arxiv.org/abs/1910.01108\n",
        "- DistilBert Tensorflow Documentation: https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GZfFboF85rIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9609ffc4-1159-4472-e43f-3d906eee5cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  66362880   ['input_3[0][0]',                \n",
            " ilBertModel)                   ast_hidden_state=(N               'input_4[0][0]']                \n",
            "                                one, 25, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_distil_bert_model_1[0][0]'] \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 256)          196864      ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 256)          0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          32896       ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           8256        ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,600,961\n",
            "Trainable params: 238,081\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(base_model, trainable=False, params={}):\n",
        "    # TODO: build the model, with the option to freeze the parameters in distilBERT\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "    # Hint 1: the cls token (token for classification in bert / distilBert) corresponds to the first element in the sequence in DistilBert. Take a look at Figure 2 in BERT paper.\n",
        "    # Hint 2: this guide may be helpful for parameter freezing: https://keras.io/guides/transfer_learning/\n",
        "    # Hint 3: double check that your number of parameters make sense\n",
        "    # Hint 4: carefully consider your final layer activation and loss function\n",
        "\n",
        "    # Refer to https://keras.io/api/layers/core_layers/input/\n",
        "    max_seq_len = 25\n",
        "    inputs = Input(shape = (max_seq_len,), dtype='int64')\n",
        "    masks  = Input(shape = (max_seq_len,), dtype='int64')\n",
        "\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    dbert_output = base_model(inputs, attention_mask=masks)\n",
        "    # dbert_last_hidden_state gets you the output encoding for each of your tokens.\n",
        "    # Each such encoding is a vector with 768 values. The first token fed into the model is [cls]\n",
        "    # which can be used to build a sentence classification network\n",
        "    dbert_last_hidden_state = dbert_output.last_hidden_state\n",
        "\n",
        "\n",
        "    # Any additional layers should go here\n",
        "    # use the 'params' as a dictionary for hyper parameter to facilitate experimentation\n",
        "    \n",
        "    dense_layer = Dense(params['dense_l_1'],activation='relu')(dbert_last_hidden_state[:, 0, :])\n",
        "    dropout_layer = Dropout(params['dropout_l_1'])(dense_layer)\n",
        "    dense_layer = Dense(params['dense_l_2'],activation='relu')(dropout_layer)\n",
        "    dropout_layer = Dropout(params['dropout_l_2'])(dense_layer)\n",
        "    dense_layer = Dense(params['dense_l_3'],activation='relu')(dropout_layer)\n",
        "    probs = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(params['regularizer_L2']), name=\"output\")(dense_layer)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs, masks], outputs=probs)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "params={# add parameters here\n",
        "        \"dense_l_1\": 256,\n",
        "\t      \"dense_l_2\": 128,\n",
        "        \"dense_l_3\": 64,\n",
        "        \"dropout_l_1\": 0.3,\n",
        "        \"dropout_l_2\": 0.3,\n",
        "        \"regularizer_L2\":0.001 \n",
        "        }\n",
        "\n",
        "model = build_model(dbert_model, params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z3EyvQbSzu5m"
      },
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "    # TODO: compile the model, include relevant auc metrics when training\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "    METRICS = [\n",
        "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "          tf.keras.metrics.Precision(name='precision'),\n",
        "          tf.keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        "    optimizer = keras.optimizers.Adagrad(learning_rate=2e-3)\n",
        "    loss = keras.losses.binary_crossentropy\n",
        "    model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=METRICS)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = compile_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Nz8kT3f8zykl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39112ed-daca-40b8-9829-61fd51f5e8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "67/67 [==============================] - 28s 272ms/step - loss: 0.6207 - accuracy: 0.7030 - precision: 0.7041 - recall: 0.9975 - val_loss: 0.6216 - val_accuracy: 0.6910 - val_precision: 0.6910 - val_recall: 1.0000\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 16s 240ms/step - loss: 0.6178 - accuracy: 0.7040 - precision: 0.7041 - recall: 0.9997 - val_loss: 0.6208 - val_accuracy: 0.6910 - val_precision: 0.6910 - val_recall: 1.0000\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 16s 240ms/step - loss: 0.6145 - accuracy: 0.7042 - precision: 0.7043 - recall: 0.9998 - val_loss: 0.6204 - val_accuracy: 0.6910 - val_precision: 0.6910 - val_recall: 1.0000\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 16s 240ms/step - loss: 0.6173 - accuracy: 0.7039 - precision: 0.7041 - recall: 0.9995 - val_loss: 0.6207 - val_accuracy: 0.6910 - val_precision: 0.6910 - val_recall: 1.0000\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 16s 240ms/step - loss: 0.6146 - accuracy: 0.7042 - precision: 0.7043 - recall: 0.9998 - val_loss: 0.6205 - val_accuracy: 0.6910 - val_precision: 0.6910 - val_recall: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, model_inputs_and_masks_train, model_inputs_and_masks_val,\n",
        "    y_train, y_val, batch_size, num_epochs):\n",
        "    # TODO: train the model\n",
        "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
        "\n",
        "    history=model.fit([model_inputs_and_masks_train['inputs'],model_inputs_and_masks_train['masks']],y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=num_epochs,\n",
        "                      validation_data=([model_inputs_and_masks_val['inputs'],model_inputs_and_masks_val['masks']],y_val))\n",
        "\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, train_model_inputs_and_masks, val_model_inputs_and_masks, train_y, val_y, batch_size=128, num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xid8Xf2naNZW"
      },
      "source": [
        "# Further exploration (REMOVE ALL CODE AFTER THIS CELL BEFORE SUBMISSION)\n",
        "Any code after this is not evaluated, and must be removed before submission.\n",
        "Leaving code below will result in losing marks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "20310675_Agrawal_Palak_ex3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}